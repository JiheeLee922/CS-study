DAY6
프로세스간 커뮤니케이션(Inter Process Communication)
프로세스간에는 직접적으로 커뮤니케이션 하는 방법은 없다. 원칙적으로 금지라서 제한해놓음
-> 서로 공간을 접근할 수 있으면 프로세스 데이터,코드가 바뀔 수 있으니 위험!

프로세스간 통신은 필요하다!  성능을 높이기 위해 여러 프로세스를 만들어서 동시 실행. 이때 프로세스간 상태 확인 및 데이터 송수신 필요

cf) fork()시스템콜
- 이 함수로 프로세스 자신(부모프로세스)을 복사해서 새로운 프로세스(자식프로세스)로 만들수 있음.
- 프로세스를 fork()해서 , 여러 프로세스를 동시에 실행시킬 수 있음.
  (최근엔 CPU안에 코어가8개되는 경우도 많고, 각 프로세스를 각 코어에 동시 실행가능(병렬처리)) 8초->1초
ex) 웹서버에 요청이 오면 HTML파일을 클라이언트에 제공하는데 , 새로운 사용자 요청이 올때마다 fork()함수로 새로운 프로세스 만들고, 즉시대응
      ->CPU가 병렬처리 가능하면, 더빠른대응 가능 / 단 이때 각 프로세스 제어 및 상태정보 교환을 위해 프로세스 간 통신 필요.     

IPC (Inter Process Communication) 기법
프로세스간 통신 방법을 제공.
- file 사용하는 방법 : 간단히 다른 프로세스에 전달할 내용을 파일에 쓰고, 다른 프로세스가 해당 파일을 읽으면 됨. 어떤 프로세스라도 저장매체는 접근가능
  But, 실시간 전달 어려움. 해당 프로세스가 계속 실시간으로 읽고만 있을 수 없어서.. 비효율적이다. 또 저장매체는 메모리에 비해 시간↑

프로세스 공간에서 할당할 수 있는 주소는 0~4GB의 주소. 여기서 3~4GB는 OS코드. 0~3GB는 여러분이 작성한 코드
(4GB다쓰는거 아니구 그중에 일부만 쓰고, 다 CPU에 들어가는 것도 아님.)
여기서 3~4GB 공간은 프로세스 a,b가 실질적으로 물리메모리(커널공간)에 동일한 공간에 접근가능. 공유가능 ->접근시간도 저장매체에 비해 짧다.
=> 대부분 IPC기법은 커널 공간을 활용한다!
ex) Message Queue , Shared Memory, Pipe, Signal, Semaphore, Socket,.....

IPC기법 종류
1. 파이프 (pipe)
단방향 통신 / fork()로 자식 만들어서 부모->자식간의 통신.
if( pipe(fd) < 0 )                           : pipe도 시스템콜.  파이프가 정상적으로 생성되면 fd에 특정한 주소값을 2개 준다. fd[1], fd[0]에!
pid = fork();                                 : fork()의 리턴값은 구분값인데 부모(>0)랑 자식(0)이랑 다름
if(pid>0) 
    write( fd[1], msg, MSGSIZE );     : 부모는 무조건 fd[1]! . 여기에 메세지를 write
else
    nbytes = read( fd[0], buf , MSGSIZE );       : 자식은 fd[0]. read를 하여서 buf에 넣기. fd[1]의 값을 갖고올수있다.
데이터가 전달되는 실제 데이터는 파일이아닌 커널공간어디에 있다~

2. 메시지 큐 (Message Queue)
큐니깐, 기본적으로 FIFO정잭으로 데이터 전송
프로세스A
msqid = msgget(key, msgflg) // key는 1234, msgflg는 옵션
msgsnd(msqid, &sbuf, buf_length, IPC_NOWAIT)  //sbuf의 값이 큐에 들어간다.
프로세스B
msqid = msgget(key, msgflg) // key는 동일하게 1234로 해야 해당 큐의 msgid를 얻을 수 있다.
msgrcv(msqid, &rbuf, MSGSZ, 1, 0)     //받은 값이 rbuf에 write.
- 메세지 큐는 부모자식이 아니라, 어느 프로세스간에라도 데이터 송수신이 가능!
- 먼저넣은 데이터가 먼저 읽혀진다.
- 양방향 가능

3. 공유메모리 (shared memory)
-노골적으로 kernel space에 메모리 공간을 만들고, 해당 공간을 변수처럼 쓰는 방식.
- 메시지큐처럼 FIFO가 아니라 , 해당 메모리 주소를 마치 변수처럼 접근.
-공유메모리 Key를 갖고, 여러프로세스가 접근가능
shmid = shmget((key_t)1234, SIZE, IPC_CREAT|0666))            : 키를 갖고 공간생성
shmaddr = shmat(shmid, (void *)0, 0)                                    : 주소얻기
strcpy((char *)shmaddr, "Linux Programming")                      : 공유메모리에 쓰기.
printf("%s\n" , (char *)shmaddr)                                              : 공유메모리에서 읽기.

=> 셋다 커널 공간을 활용해서 프로세스간 데이터 공유


DAY7
IPC 기법 시그널과 소켓
시그널과 소켓은 ipc기법을 위해서 만들어진건 아닌데 그 원래 목적 이외에도 프로세스간 통신이 가능해서 쓴다.

시그널(signal) ->이벤트
- 유닉스에서 30년 이상 사용된 전통적인 기법
- 커널(또는 프로세스) 가 이런 처리를 해줘라고 프로세스에 알려줄 메커니즘이 필요한데 그 때 시그널을 쓴다.
- 미리 정의가 되어있는 이벤트 라고 보면 됨. 종류는 64개정도 있다. 무조건 대문자로 정의되어있음.
- 각각의 시그널을 프로세스에서 받게되면 처리해야하는 동작들을 OS단에서 다 정의해놨다.
ex) SIGKILL : 프로세스를 죽여라. / SIGUSR1, SIGUSR2 얘네는 기본동작 정의안되어있음. 특별한 동작을 하도록 만들 수 있다. 프로세스간에 이거 보내면서 ipc기법처럼 통신가능

static void signal_handler(int a){  printf(a); }
signal(SIGINT, siganl_handler)            : SIGINT시그널을 받으면 기본동작이 아닌 sinal_handler 함수 수행해라
signal(SIGINT, SIG_IGN)                      : SIGINT 시그널 받으면 기본동작하지말고 걍 무시해라(SIG_IGN)

- 시그널은 항상 커널모드에서 사용자모드로 전환할 때 해당 프로세스 PCB에 가서 대기중인 시그널 처리. 
  이 때, 사용자모드 못 가고 다시 커널모드에서 커널함수 실행할수도 있구...

소켓(socket)
- 코드가 굉장히 복잡하다.
- 원래는 네트워크 통신을 위한 기술
- 클라이언트와 서버 사이의 네트워크 기반 통신기술이다. 네트워크 기기를 이용할 수 있는 시스템콜
- 굳이 컴퓨터 간이 아니라 컴퓨터 안의 프로세스간에도 가능 해서 ipc기법처럼 쓴다.
- 소켓처리는 커널모드에서한다.


총정리 : 코드를 컴파일 하면 실행파일이 만들어진다. 일반 사용자는 이 실행파일을 실행하기 위해 쉘(cli, gui)을  사용하기 위해 OS에 요청. 
실행파일은 OS에서 어떻게 동작하지? 일단 프로세스가 구성된다. 컴파일된 코드는 TEXT영역에 들어간다. new->ready 상태가 되고, 스케쥴러에 따라 ready->running 상태가 된다. 만약 선점형 스케쥴러 방식으로 0.05초마다 바꿔준다면, 타이머 인터럽트가 0.01초마다 한다고 하면, 타이머 인터럽트가 발생하면 커널모드로 바꿔주고 IDT로 가서 타이머 인터럽트에 해당하는 번호에 매칭된 커널함수의 주소값 확인하고 그 함수 실행.
예를들어 그 함수가 sys_timer{count++; if count>4 스위칭;} 라면 5번하면 컨텍스트 스위칭( PCB정보 cpu에 넣어주고 이제 내코드 cpu에서 실행 )이 되어서 running상태가 된다.  
실행하다가 코드안의 open()시스템콜 함수 실행 
mov eax,1      : eax에는 시스템콜 번호를 넣어주고
mov ebx,0      : 시스템콜 인자(파라미터)
int 0x80          : 인터럽트 번호 전송
ㄴ int는 cpu가 제공하는 OPCODE. int라는 명령 자체가 사용자모드를 커널모드로 바꿔주고 IDT 에서 해당 함수 호출.
sys_open()함수 실행하면 이제 waiting상태가 된다. DMA가 데이터 다 받아오고 다 처리 됐다 이러면 또 인터럽트 발생해서 wait -> ready 상태로.
또 타이머 인터럽트 5번 발생하면 ready-> running되고 그다음 코드 줄 실행
=>사용자모드에서 커널모드로 수시로 이동. 수많은 인터럽트와 시스템콜 발생

IO관련 처리 -> 수많은 인터럽트, 시스템콜, 스케쥴러, H/W
빈번한 IO처리는 엄청난 오버헤드. 시간이 많이든다. 보통 IO처리가 있다면 한번에 다 처리하려고 노력


Thread (스레드)
- 프로세스처럼 동작하지만 그거보단 구조가 작다. Light Weight Process라고 함.
- 프로세스의 서브넷
- 하나의 프로세스에 여러개 존재가능.
- 스레드는 IPC사용할 필요가 없다. 하나의 프로세스 안에 있으므로, 프로세스의 데이터 모두 접근가능함.
- 각각의 stack 영역을 갖고있는 함수라고 생각하면 됨. (정확히는 stack과 heap 중간에 존재)
- 각 스레드를 위한 stack공간이 존재 & 각 스레드를 위한 SP,PC도 존재.
- 각 스레드는 register,stack만 따로있고 heap, bss, data, code는 다 동일한거니깐 공유가능.
- 다른 프로세스안의 스레드랑은 당연 공유안됨.

 스레드1을 실행한다고 할 때, 동일한 프로세스를 실행하긴 하되, 스레드1에 있는 SP,PC를 갖고 , 스레드1에 있는 스택영역을 기반으로 해서 코드가 움직인다.

멀티프로세싱 : 여러 CPU , 여러 프로세스, 병렬실행 , 실행속도 높임
하나의 프로세스를 여러 CPU인 경우는 하나의 프로세스 안에 thread를 여러개 만들어서 여러 CPU에 실행하는 것!
스레드는 CPU멀티코어환경에서 실행속도 높이는데 굉장한 기능한다.

스레드 장점
- 각 클라이언트 요청에 대응하기 위해 별도의 스레드(프로세스)를 생성해서 대응. 응답성 향상
- 프로세스는 별도의 공간이 있고 공유x. 스레드는 하나의 프로세스안에서 자원공유하니깐 자원공유효율높다.
    ㄴ프로세스6개면 24GB , 스레드6개면 그냥 4GB
- 작업이 분리되어 코드가 간결(뭐 하기나름이다.)
스레드 단점
- 스레드 중 하나만 문제있어도, 전체 프로세스가 영향받는다.
- 스레드가 많이 생성되면 모든 스레드를 스케쥴링 해야하므로 context switching이 빈번하여 성능저하


Day8
Thread 동기화 이슈(synchronization)
스레드 a, b, c의 실행순서는 정해져있지않고 스케쥴러가 한다. 근데 여기서 순서가 꼬이게 되면 비정상 동작을 하는경우가 있는데 이게 동기화 이슈
-> 꼬이면 디버깅관리도 쉽지않고 하므로 스레드 관리가 필요하다.
- 동일자원을 여러 스레드가 동시 수정 시 , 각 스레드 결과에 영향을 줌.

동기화 : 작업들 사이에 실행시기를 맞추는 것.

동기화 이슈 해결방안
Mutual exclusion (상호 배제)
- 여러 스레드가 변경하는 공유 변수에 대해 상호배제가 필요.
- 어느 한 스레드가 공유변수를 갱신하는 동안 다른 스레드가 동시 접근 못하도록 막는 것.
- 임계자원(critical resource) : 동시에 수정되면 안되는 변수. 자원. 
- 임계영역(critical section) : 동시실행을 하면 안되는 코드. 영역

lock.acquire()               -> lock이라는 함수를 이용하여 acquire로 열쇠를 얻기
for i in range(10000):
    g_count += 1
lock.release()               -> release로 열쇠 반환
ㄴ여기서 for문이 임계영역이고 g_count가 임계자원이다.
ㄴ 열쇠가 없는 스레드는 기다리고, 실행중인 스레드가 release 하면 그때 열쇠 얻는 acquire써서 임계영역 실행

 임계구역에 대한 접근을 막기위해 locking 메커니즘(=상호배제)이 필요.
ㄴ Mutex( binary semaphore) 기법 ( 임계구역에 하나의 스레드만 들어갈 수 있음 ) 과 세마포어기법이 있다.

세마포어(Semaphore)
- 전통적인 리눅스에서 구현되어있는 하나의 함수.
- 임계구역에 여러 스레드가 들어갈 수 있음.
- counter를 두어서 동시에 리소스에 접근 할 수 있는 허용 가능한 스레드 수를 제어

P : 검사 (임계영역 들어갈 때) -> S값이 1 이상이면, 임계영역 진입 후 S값 1차감( S값 0이면 대기) lock.acquire()
V : 증가 (임계영역에서 나올 때 )-> S값을 1 더하고, 임계 영역을 나옴 lock.release()
S : 세마포어 값 (초기 값만큼 여러 프로세스가 동시 임계영역 접근 가능) S가 2면 2개가 임계영역 들어갈수있는것.

주요 세마포어 함수 - POSIX 세마포어
sem_open() : 세마포어를 생성
sem_wait() : 임계영역 접근 전, 세마포어를 잠그고,/  세마포어가 잠겨잇다면, 풀릴 때 까지 대기(P)
sem_post() : 공유자원에 대한 접근이 끝났을 때 세마포어 잠금을 해제(V)


만약 S가 0이면 임계영역에 들어가기위해 계속 while반복문을 수행 -> 바쁜 대기 busy waiting
=>프로그래밍은 근본적으로 중단이 없음. 끊임없이 코드 실행. 중단은 대부분 loop로 표현. loop는 CPU부하를 걸리게해서 성능 떨어짐
ㄴ이 보완 기술로 대기큐가 있다.

대기큐
- 바쁜대기로 인한 cpu부하를 막기위한 운영체제 기술
- S가 음수면 바쁜 대기 대신 대기큐에 넣고 block() 
- wakeup(P) 함수를 통해서 대기큐에 있는 프로세스 재실행.

cf) pseudo code(슈도코드) : 그냥 로직을 설명하는 코드??

임계영역을 설정하고 임계영역에 스레드가 하나 접근하고 나머지 기다릴때, 그 순서를 잘못 매기면 아예 아무도 접근 못하거나, 특정 스레드만 계속 접근못하는 현상이 생길수도있다.

Thread Deadlock (교착상태)
- 두개 이상의 작업이 서로 상대방의 작업이 끝나기만을 기다리고 있기 때문에, 다음 단계로 진행하지 못하는 상태
[ThreadA]
lock.acquire(a) : a라는 임계자원 locking
     ㄴ use a lock.acquire(b)         : b라는 임계자원 locking -> b자원 요청해도 스레드B에서 쓰니 대기
          ....
lock.release(a)
[ThreadB]
lock.acquire(b)
     ㄴ use b lock.acquire(a)         :a자원 요청해도 스레드 A에서 쓰니 대기
         .....
lock.release(b)
- 배치처리시스템에선 일어나지 않는 문제(당연.)
- 프로세스, 스레드 둘다 이 상태가 일어날 수 있음.
- 교착상태 발생조건
    1. 상호배제(Mutual exclusion) : 프로세스들이 필요로 하는 자원에 대해 배타적인 통제권을 요구
    2. 점유대기(Hold and wait) : 프로세스가 할당된 자원을 가진 상태에서 다른 자원을 기다린다.
    3. 비선점(No preemption) : 프로세스가 어떤 자원의 사용을 끝날 때까지 그 자원을 뻇을 수 없다.
    4. 순환대기(Circular wait) : 각 프로세스는 순환적으로 다음 프로세스가 요구하는 자원을 가지고 있다.
->4가지 모두 성립해야함 => 하나라도 없애면 교착상태 막기 가능.

Starving 기아상태
특정 프로세스의 우선순위가 낮아서 원하는 자원을 계속 할당 받지 못하는 상태
ex) 스레드가 50개인데 49개는 우선순위1번, 1개는 우선순위 2번일때, 실행은 스케쥴링으로 10번만 하고 프로그램 종료.
          -> 2번 우선순위를 가진 1개는 그 10번안에 절대못드니 절대 실행x
- 해결방안 : 우선순위를 수시로 변경 / 오래기다린 프로세스의 우선순위 높이기 / 우선순위가 아닌 FIFO기반 요청큐로 처리

=> 교착상태는 여러 프로세스가 동일자원점유 요청할때 발생 / 기아상태는 여러프로세스가 부족한 자원을 점유하기위해 경쟁할때 발생



가상메모리 (Virtual Memory System)
실제 각 프로세스마다 충분한 메모리를 할당하기에는 메모리 크기가 한계가 있다. 하나의 프로세스는 4GB인데 통상 메모리는 8,16Gb이다. 코드는 무조건 메모리에 있어야 하는데 너무 부족하니 가상메모리 사용. 어떻게 하면 적은 메모리로 여러프로세스를 실행할수 있을가 해서 나온거.
- 메모리가 실제 메모리 보다 많아 보이게 하는 기술
- 프로세스는 가상 주소를 사용하고, 실제 해당 주소에서 데이터를 읽고/쓸때만 물리주소로 바꿔주면 된다.
- virtual address : 가상주소. 프로세스가 참조하는 주소 (0~4GB)
- physical address : 물리주소. 실제 메모리 주소. (여긴느 0~4GB 중 일부만 메모리에 올라간다.)
- 지금 당장 cpu가 쓸 공간만 메모리에 넣어주고 빼고 한다. 그 물리 메모리 공간에 어딨는지만 알면 여러프로세스를 작은 메모리에서도 실행하지 않나? 그 메커니즘이 가상메모리!

메인메모리에는 실제 각 프로세스의 데이터가 조각조각으로 씌여져있다. 

CPU가 어떤 특정프로세스의 공간을 참조할 때는 가상주소를 먼저 찾게되고,  그 가상주소가 실제 어느 물리주소인지 알아야돼서 가상주소를 물리주소로 변환해주는 메커니즘이 CPU에 있다. 근데 그 변환 시간이 많이 걸리니깐 MMU라는 하드웨어 칩을 넣어준다.
-> 가상메모리는 MMU라는 하드웨어 지원이 필요하다 

MMU (Memory Manangement Unit) 
- CPU에 코드 실행시, 가상주소메모리 접근이 필요할 때 해당 주소를 물리 주소값으로 변환해주는 장치 
- CPU는 가상메모리만을 다루고, 실제 해당 주소 접근 시 MMU 하드웨어 장치를 통해 물리메모리 접근 (빠르다.) 


페이징 시스템(Paging system)
- 가상메모리 시스템에서 가장 많이 쓰이는 메커니즘
- 물리메모리에 어느 사이즈만큼 올려줘야할지 모르니간 페이지 단위로 올리자!
- 페이징 
          -크기가 동일한 페이지로 가상주소공간과 이에 매칭하는 물리주소공간을 관리
          -하드웨어의 지원이 필요 (ex) Intel x86시스템(32bit)에서는 4KB,2MB,1GB지원 )
          -리눅스에선 4KB(단위)로 paging
          -페이지 번호를 기반으로 가상주소/물리주소 매핑 정보를 기록/사용
- 실제 프로세스가 4GB면 다 4KB씩 쪼개서 각각의 단위를 페이지
   각각의 페이지에 번호를 붙인다. 페이지 단위로 그 데이타를 물리메모리에 넣고, 찾을때도 페이지단위로 해당주소를 찾게된다.
- Page Table : 페이지 각각의 번호별로 가상주소와 그 해당하는 물리메모리 주소를 매핑해놓은거! (프로세스의 PCB안에있다.)
- page는 고정된 크기의 블럭
    만약 9kb인걸 4kb씩 나눈다면 4kb, 4kb, 1kb 이렇게 나눠지니깐 page3은 (1kb + 3kb의 공란)이다.
    이 공란포함 4kb가 물리메모리에 들어가게 된다. 그리고 그 물리메모리 주소를 page table에 넣는다.

페이징시스템구조
가상주소v 가 32bit면 32비트를 쭉 나열하면 31~11은 페이지번호 , 11~0 은 뭐지?변위다
  ㄴ   사실 4kb로 구성하다보니 내가 실제 접근할 코드 한줄은 4kb인 블럭주소 맨위에서 얼마정도 떨어져있을수 있다. 만약 2kb면 page번호의 해당하는 맨위의 base주소 + 2kb하면 그 코드 주소 알수있다. 여기서 2kb가 변위(d)
-> 가상주소의 0비트에서 11비트가 변위를 나타내고, 12비트 이상이 페이지 번호가 될 수 있음.

페이지 가상주소에 대한 물리주소만 기억하면 거기에 변위를 더하면 실제 물리메모리에있는 해당데이터의 위치를 알수있는셈!



페이지테이블
물리주소에 있는 페이지번호와 해당페이지의 첫물리 주소 정보를 매핑한 표
페이징시스템 동작 -> 해당 프로세스에서 특정 가상 주소 액세스 하려면 해당 프로세스의 page table에 PCB에 pagetable 맨 첫주소(base주소)를 갖고와서 여기에 페이지 번호만 알면 이 페이지 테이블에서 해당페이지에 매칭되는 물리주소를 알고 물리주소로 실제 메모리에 들어갈때 그 물리주소+변위 하면 그 코드 값을 알아낸다.
물리메모리에 저장되는 주소들은 페이지 순서랑 관계없음
                   Process1                                                                 Process1 Page Table                                   Physical Address


그림보면 내가 aspect데이터 접근하려면 cpu가 요청한다면 가상주소 00000Ah에다가 몇번째인지 덧붙여서 보내주면 앞에있는 주소를 보면 page3인걸알게되고 PCB가서 pagetable에 물리주소가 1000h라서 찾아가고 1000h에서 변위값은 3인걸 아니간 3더해주면 데이터 찾는다. physical address는 필요한 데이터만 넣어놓는거 필요없으면 없애기!

page table에는 valid-invalid bit 라는 컬럼이있는데 물리주소에 지금 그 데이터가 있는지 체크해주는거. v면 들어잇는거,i면 안들어가잇는거

- 페이징 시스템과 MMU 
PCB에서 해당페이지 테이블 접근이 가능하다. 프로세스가 구동 시  PCB관련정보(pageTable)는 물리메모리에 적재되고 , 해당 페이지 테이블 base 주소가 별도 레지스터에 저장된다(CR3라는!) 
그 후에 cpu가 해당 프로세스의 가상주소를 실행하기위해 접근을 하면 MMU가 CR3레지스터 갖고와서 페이지테이블 참조해서 물리주소로 변환한다음 그 물리주소를 갖다준다.
==>>어려워ㅜㅜㅜㅜ잘이해가 안된다ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ============


Day9
다중 단계 페이징 시스템
미리 앞단에 큰 영역별로 구분질 수 있는 페이지 디렉토리를 만들고 안쓰는 영역은 페이지테이블 안 만들고, 쓰는 것만 영역별로 만든다. 
->공간 절약 가능

32bit 시스템에서 4KB페이지를 위한 페이징 시스템은
하위 12bit는 오프셋(OFFSET = 변위) / 상위 20bit가 페이징 번호. 2의20승개의 페이지 정보가 원래 필요
->상위 20bit중 상단의 10bit는 페이지 디렉토리를 나타내는 비트로 만들고 , 페이지 디렉토리 중에 실제로 쓰는 쪽에 페이지 테이블을 표시
=>32bit중 하위12bit는 변위, 상위 20bit중 상단 10bit는 페이지 디렉토리, 나머지 10bit는 page table .이렇게 3단계 (리눅스에서, 최근은 4단계까지 더 나눔)

CR3레지스터(page directory의 시작주소 갖고있는)가 해당 주소에 대응하는 디렉토리에 가서 해당 디렉토리에 대한 페이지 정보를 갖고 있는 페이지 테이블의 맨 앞의 주소를 갖고 페이지 테이블에서 해당 페이지 번호를 찾아서 , 그 페이지번호와 offset 정보를 갖고 물리메모리로 가서 정보 갖고온다.


CPU가 데이터를 요청할 때 , virtual address를 갖고 MMU에 요청한다. MMU는 CR3레지스터에 있는 base address를 갖고 메모리에 있는 page table에 가서 physical address갖고오고, 그 물리주소를 갖고 다시 또 메모리로 가서 page를 찾아서 data를 갖고와서 CPU전달시킨다
-> 메모리를 갔다오는 시간은 꽤 걸리는데 MMU가 2번이나 갔다오게 된다. 
=> 별도의 캐쉬 보조 하드웨어 쓴다. TLB
TLB(Translation Lookaside Buffer) : 페이지 정보 캐쉬
MMU가 한번 접근해서 변환이 된 물리주소를 TLB에 저장해서 다음에 또 가상주소 요청이 오면 TLB에서 먼저 찾고 , 있으면 그걸로 써서 메모리 바로 접근.
메모리 접근횟수가 줄어든다.


공유메모리
- ipc에서도 한번 다뤘었음.  커널영역에서 만들어지는 공간.
1. 두 프로세스가 데이터를 공유한다면 물리메모리에 하나만 두고 둘이 같이 쓰는 것. 각각의 page table에 똑같은 물리메모리 주소만 들어가면 된다.
2. fork()로 자식 프로세스를 만들었을 때, 4GB가 다 생기지 않음(만약 그러면 엄청난 공간낭비) . 물리메모리에 있는 주소만 자식 프로세스의 page table에 넣고 쓰면 된다. 어차피 복제한거니깐 똑같아서! 
-> 만약 자식 프로세스에서 수정작업이 있다면, 그 때 물리메모리에서 쓸 페이지를 복사를 하고 자식 프로세스의 page table에서 주소만 바꾸면 된다.
=> 프로세스 생성시간 엄청 짧아짐.  공간도 효율적으로 쓸 수 있다.


요구페이징(Demand Paging)
프로세스 모든데이터를 메모리로 적재하지 않고, 실행 중 필요한 시점에서만 메모리로 적재.
더이상 필요치 않은 페이지 프레임은 다시 메모리에서 저장매체에 저장 -> 페이지 교체 알고리즘 필요(Page fault)
<-> 선행페이징(anticipatory paging) : 미리 프로세스 관련 모든 데이터를 메모리에 올려놓고 실행

Page fault 인터럽트
- 어떤 페이지가 실제 물리메모리에 없을 때 일어나는 인터럽트.
- 운영체제에 인터럽트 날려주면, OS가 해당 페이지를 물리메모리에 올려주고, page table에서 valid-invalid-value도 v로 바꿔준다.
ex) CPU가 데이터를 요청할 때 , virtual address를 갖고 TLB에 캐쉬 있는지 보고 있으면 바로 메모리 접근해서 page찾아서 데이터 전달. 없으면, MMU에 요청한다. MMU는 CR3레지스터에 있는 base address를 갖고 메모리에 있는 page table에 가서 invalid-valid-value가 invalid면, OS에 page fault 인터럽트를 발생. 그럼 IDT로 가서 시스템콜 함수 찾고 실행... 저장매체에서 page를 찾고, 그 page를 메모리에 올려준다. 그리고 page table을 i->v로 업뎃한다. 그리고 page table에서  physical address갖고오고, 그 물리주소를 갖고 다시 또 메모리로 가서 page를 찾아서 data를 갖고와서 CPU전달시킨다

-페이지 폴트가 자주 일어나면 시간이 오래걸리게 된다. 그렇다고 안일어나게 하려면 향후 실행될 데이터,코드를 예측해야하는데 불가능

페이지 교체 정책(Page replacament policy)
OS가 특정 페이지를 물리메모리에 올리려고 하는데 물리메모리가 다 차있다면, 
기존 페이지중 하나를 물리메모리에서 저장매체로 저장시키고, 새로운 페이지를 해당 물리메모리 공간에 올린다
-> 어떤 메모리를 물리메모리에서 저장매체로 내리지? -> Page Replacement(Swapping) 알고리즘 필요하다.

1. FIFO Page Replacement Algorithm : 가장 먼저 들어온 페이지를 내리는 것.
2. OPTimal Replacement Algorithm : 최적 페이지 교체 알고리즘. 앞으로 가장 오랫동안 사용하지 않을 페이지 내리는 것. 일반 OS에선 구현 불가
-> OPT는 실현 불가니깐 조금이라도 OPT랑 비슷하게 하려고 나온 알고리즘 사용. 
3. LRU(Least Recently Used)  
    - 가장 오래 전에 사용된 페이지를 교체
    - opt는 불가하니, 과거 기록을 기반으로 시도한 알고리즘
    - 제일 많이 쓰고, 그나마 opt와 유사한 성능. 메모리지역성(앞뒤의 코드를 제일 많이 쓴다는) 때문에
4. LFU(Least Frequently Used) : 가장 적게 사용된 페이지 내리는 것.
5. NUR(Not Used Recently)
    - LRU와 마찬가지로 최근에 사용하지 않은 페이지 부터 교체.
    - 각 페이지마다 참조비트(read)와 수정비트(modify)를 둔다 (R,M) 이렇게 pair로! 
    - (0,0), (0,1),(1,0),(1,1) 순으로 페이지 교체

cf) 스레싱 (Thrashing ) : 반복적으로 페이지폴트가 발생해서, 과도하게 페이지 교체작업이 일어나, 실제로는 아무일도 하지 못하는 상황. 동작x


세그멘테이션(Segmentation) 기법(페이징시스템이랑 비교해서만 알아두기)
- 가상 메모리를 서로 크기가 다른 논리적 단위인 세그먼트로 분할 ( 페이징시스템은 같은 크기의 블록으로 분할 )
- Intel x86 리얼모드에서 사용. CS(code), DS(data), SS(stack), ES(extra) 세그먼트를 나눠 메모리에 접근

cf) x86 리얼모드 : Inte CPU 80286 에서 보호모드(protection ring)지원하기 시작. 그럼 kernel/ user mode도 만들어짐. 그래서 기존의 8086/ 80186 CPU에서 제작된 소프트웨어가 실행되기 어려워졌다. 이걸 호환시키기 위한 호환모드가  리얼모드이다. 리얼모드는 최대 1GB의 메모리 공간을 쓸 수 있다. 그 공간을 세그먼트로 나눠서 메모리에 접근한다.

cf) 
내부 단편화 : 페이징시스템일 때, 4kb씩 넣을때 1kb로 나눠지는 애는 쓰지도 않는 3kb와 같이 메모리에 저장되어야 하니간 공간낭비
외부 단편화 : 세그멘테이션 기법일 때, 물리 메모리가 원하는 연속된 크기의 메모리를 제공하지 못하는 경우
-> 세그멘테이션/페이징 모두 하드웨어 지원 필요.
-> 다양한 컴퓨터 시스템 이식성을 중요시하는 리눅스는 페이징 기법을 기반으로 구현한다.

가상메모리 총정리
코드를 컴파일하면 실행파일(1kb)가 만들어진다. 이 코드가 실제 쉘을 통해서 os에서 실행하면 프로세스가 만들어진다. 그 프로세스의 가상메모리 영역은 4GB.  가상메모리에서 0~3GB는 사용자영역, 3~4GB는 커널영역이다.  코드는 가상메모리의 text 영역에 들어간다. 그 4GB의 영역은 보통 4KB의 페이지로 조각 나눠지고 page directory에서 필요한 영역만 페이지 테이블 생성한다.
커널영역이 1GB씩 항상 붙는데 공간낭비? 따지고 보면 공유가능해서 낭비는 아님. 추가적인 공간이 든다거나 하지 않는다.
코드가 올라가서 가상메모리가 만들어졌을 때 실제 실행 전에는 관련된 코드가 물리메모리에 바로 적재x

-> lazy allocation : 실행파일도 SSD/하드디스크 저장메체에 있다. 저장매체에 있는 데이터를 읽어서 물리메모리에 올리는건 시간이 많이거린다. 실행파일이 가상메모리화 돼서 프로세스가 만들어지는데 그게 실행되어서 꼭 그 데이타가 필요한 시점이 되기 전에는 어느 데이터도 파일에서 물리메모리에 직접 넣지 않는다. 시간이 오래걸리니깐 ! 실행해 라고 하면 해당 페이지를 물리메모리에 올린다. (요구페이징기법) -> 페이징 폴트 -> 실제 코드를 물리메모리에 올리고 page table 업뎃. 빨리하려고 TLB 하드웨어 지원도 받는다.


Day10
파일 시스템 
- 운영체제가 저장매체에 파일을 쓰기 위한 자료구조 또는 알고리즘.
- 만들어진 이유 ?
     원래는 0과1의 데이터를 bit단위로 저장매체에 저장함.
     -> 오버헤드가 너무 큰 단점-> 보통 4KB의 블록단위로 관리하기로 함
     -> 블록마다 고유번호를 부여해서 관리했지만 사용자가 각 블록 고유번호를 관리하긴 어려움
     -> 사용자가 이해하기 쉬운 추상적(논리적) 객체 필요 -> 그게 파일! ->사용자는 파일단위로 관리 (각 파일은 블록단위로 관리)
     -> 파일을 저장매체에 효율적으로 저장하려면 가능한 연속적인 공간에 저장하는게 좋음
     -> but, 파일 사이즈가 가변적이라 불연속 공간에 파일 저장 기능 필요
     -> 블록체인 : 블록을 링크드 리스트로 연결 but, 끝에 있는 주소 찾으려면, 맨첨 블록부터 주소 따라가야함
     -> 인덱스 블록 기법 : 각 블록에 대한 위치 정보를 저장해서, 한번에 끝 블록 찾아 갈 수 있는 기법
- Window 의 파일 시스템 : FAT, FAT32 , NTFS(요즘에 많이 쓰는 것) - 블록위치를 FAT라는 자료구조에 기록
- Linux의 파일 시스템 : ext2, ext3, ext4 - 일종의 인덱스 블록 기법인 inode 방식 사용
- 각 파일시스템은 동일한 시스템콜 사용가능. 다 지원되므로 우리는 굳이 시스템콜 쓸때 어떤 파일시스템인지 고려할 필요x.

inode 방식 파일 시스템
- 파일이 생성되면 inode 번호가 생기고 , inode 블록이 생겨서 inode 블록 기반으로 파일 처리.
    (≑ 프로세스가 생성되면 processID가 생기고 id 기반으로 PCB정보가 만들어지고 그걸 사용해서 스케쥴링 등 여러 작업 )
- 기본 구조
    * 수퍼 블록 : 파일 시스템 정보(대표정보) 및 파티션 정보 포함 .  df라는 명령어로 출력가능
    * 아이노드 블록 : 파일 상세정보(≑PCB)
    * 데이터 블록 :  실제 데이터(보통 1~4KB)
- 파일은 inode 고유값과 자료구조에 의해 주요정보 관리.
- '파일이름:indoe'로 매칭 시키고 inode 기반으로 파일 액세스 한다.
- inode 기반 메타데이터(파일권한, 소유자정보, 파일사이즈, 생성시간등 시간관련정보, 데이터 저장위치) 저장
- Super Block(전체 파일시스템???) > inode (파일 두개 관리그림) > Disk Block (각 파일안에 여러개 있음)

cat files.txt            :  files.txt의 내용을 불러오라는 명령어
-> 파일의 inode 번호를 갖고 inode 블록을 찾아가고, 내용을 불러오라 했으니 Direct block으로 간다.
     Direct block은 12개 정도의 주소공간을 갖고있다. 각각의 12개의 공간이 실제 데이타 블록(1~4KB)의 주소를 가르키고 있다.

ls -al filies.txt         : 파일의 권한정보, 소유자정보, 사이즈 등 갖고오라는 명령어
->파일의 inode 번호를 갖고 inode 블록으로 가서 Mode, Owner Info에 있는 정보 갖고온다.


inode 구조

Direct block이 가르키는 data block은 각각 1~4kb이다. 12개가 있다고 했으니 최대 48KB밖에 안된다. 부족하다고  Direct Block을 늘리면 비효율적
-> 그래서 Indirect block(Single indirect) , Double indirect , Triple indirect 가 나왔다. 각각 1개씩 있음.

Single indirect는 4KB의 공간에 data block의 주소를 4byte씩 1024개 갖고있다. 4KB * 1024 = 4MB의 정보를 가질 수 있다.
Double indirect는 Single indirect의 블록 주소를 1024개 갖고있다. 4KB * 1024 * 1024 = 4GB의  정보를 가질 수 있다.
Triple indirect는 Double indirect의 블록 주소를 1024개 갖고있다. 4KB * 1024 * 1024 * 1024 
=> inode 블록은 간단히 작게 가지지만, 파일사이즈가 적으면 Direct block만 쓰고, 파일사이즈가 많으면 Triple indirect까지 쓸 수 있다.

디렉토리 엔트리(Directory Entry)
- 리눅스는 파일을 탐색하기 위해 디렉토리 엔트리를 탐색한다.
- 각 엔트리에는 해당 디렉토리 파일 / 디렉토리 정보를 갖고있다.
- 디렉토리 엔트리를 사용해 찾고자 하는 파일의 inode번호를 얻는다.

가상 파일 시스템 (Virtual File System. VFS)
- 파일이 아닌, Network등 다양한 기기도 동일한 파일 시스템 인터페이스를 통해 관리 가능
- 모든 것은 파일이라는 철학으로, 모든 인터렉션은 파일을 읽고, 쓰는 것처럼 이뤄져있음.
- 마우스, 키보드와 같은 모든 디바이스 관련된 기술도 파일과 같이 다루어짐.
- 모든 자원에 대한 일관적이고 단순한? 추상화 인터페이스로 파일 인터페이스를 활용

cf) 특수파일
디바이스는 내부적으로 블록디바이스/ 캐릭터 디바이스로 나뉜다.
* 블록디바이스 : HDD, CD/DVD와 같이 블록 또는 섹터 등 정해진 단위로 데이터 전송, IO 송수신 속도가 높음
* 캐릭터 디바이스 : 키보드, 마우스 등 byte 단위 데이터 전송, IO송수신 속도가 낮음


부팅 (Boot)
- 컴퓨터를 켜서 동작시키는 절차
- Boot프로그램 : OS의 커널을 저장매체에서 특정주소의 물리메모리로 복사하고, 커널의 처음 실행위치로 PC를 가져다 놓고 실행하는 프로그램
- 폰노이만 구조를 보면 메모리에 있는 코드를 갖고 CPU를 실행시키는데, 첨엔 메모리가 아예 비어있는데 어떻게 저장매체에서 커널갖고오지?밑에!

cf) ROM-BIOS  : ROM은 한번쓰면 영원히 그 데이터가 남아있음.  특별한 RAM이라 보면 된다. bios칩은 컴퓨터가 꺼져도 그 데이터가 살아있다.

부팅과정
컴퓨터가 켜지면 CPU가 ROM에 있는 특정주소를 읽게된다. 그 주소는 bios해당 프로그램이 있는 맨 앞 주소. 거기로 가면 bios 프로그램의 일부가 실행된다. 그 프로그램은 실제 bios 프로그램을 메모리에 로드시킨다. 
(그냥 ROM-BIOS에서 bios 프로그램 실행시키지 왜 메모리에 올림? ROM-BIOS칩은 특별한 칩이라 속도가 느리다. 속도빠른 RAM에 올리는 것!)
bios 프로그램 ( 컴터켜서 bios메뉴 찾아가고 하는 그거) 역할
1. 컴퓨터(Hardware) 초기화
2. 저장매체의 맨앞의 특정공간은 MBR(Master Boot Record) 이라 부르는데, 이 MBR을 찾아가서 그 특정 size의 데이터를 읽어온다 
    -> 그 데이터에는 bootstrap loader라는 프로그램이 들어있다. 이 프로그램을 메모리에 로드시킨다.
    -> 부트로더 프로그램의 처음 실행. -> 부트로더는 저장매체에있는 MBR에 있는 특정코드를 본다. 그 코드에는 파티션 table이 들어있음
(파티션 table : C:\, D:\ 이런 파티션 중 하나에 운영체제가 들어있다. 그런 정보를 가진 테이블)
    -> 파티션 테이블에서 어떤 파티션이 메인파티션인지 알아낸다.
    -> 파티션에는 부트섹터라는 공간(저장매체에) 있는데 그 공간으로 가면 부트코드라는게 있다.
    -> 부트코드를 메모리에서 읽어서 실행
    -> 부트코드는 해당 파티션안에 커널이미지(실행파일)이 있는데 그 주소를 알아내서 그 커널이미지를 메모리에 로드
    -> 그  커널 이미지의 맨앞으로 PC옮겨서 실행한다. -> 환영합니다 화면 뜬다.


가상머신 (Virtual Machine) - 가볍게 동작정도만 이해하면 됨
- 인공지능, 클라우드컴퓨팅에서 사용하는 기술.
- 하드웨어를 소프트웨어로 마치 여러개인거처럼 하는 기술.
- 하드웨어(컴퓨터)는 하난데 , 가상기계가 마치 자기가 컴퓨터인거처럼 모사해서 가상머신마다 운영체제를 설치해서 써서 컴퓨터 여러개인거처럼!
- type1, type2 / 전가상화, 반가상화로 구분된다.

Type1 (native 또는 bare metal)
- 하이퍼바이저(또는 VMM(virtual machin monitor) : 운영체제와 응용프로그램을 물리적 하드웨어에서 분리하는 프로세스 ex)Xen, KVM(AWS에서 사용)
- 하이퍼바이저 또는 VMM 소프트웨어가 Hardware에서 직접 구동 
- 2보다 성능이 더 좋다. 하드웨어 직접 액세스 하므로. 
Type2 
- 하이퍼바이저 또는 VMM 소프트웨어가 HostOS 상위에 설치. ex)Parallels Desktop, VMWare(대중적. 내pc에서 리눅스 쓰고 싶을 때)

Full Virtualization (전가상화)
- 각 가상머신이 하이퍼바이저를 통해서 하드웨어와 통신
- 하이퍼바이저가 마치 하드웨어인 것처럼 동작하므로(번역가 역할), 가상머신의 OS는 자신이 가상머신이 상태인지를 모름.
Half Virtualization (반가상화)
- 각 가상머신에서 직접 하드웨엏와 통신.
- 각 가상머신에 설치되는 OS는 가상머신인 경우, 이를 인지하고, 각 명령에 하이퍼바이저 명령을 추가해서 하드웨어와 통신
- 전가상화보다 빠르다. but, OS 수정이 필요해서 복잡하다.
- VMM은 리소스 관리한다고 생각. 각 OS에 리소스를 얼마나 할당해줄건디 이런거.

KVM에 대해 프린트에 더 자세히~!

Docker
- 가상머신은 하드웨어를 가상화(하드웨어 전체 추상화)하므로 하이퍼바이저사용, 추가 OS필요 등 성능 저하 이슈 존재
- 도커는 운영체제 레벨에서 별도로 분리된 실행환경을 제공(커널 추상화)
- 마치 리눅스 처음 설치했을 때와 유사한 실행환경을 만들어주는 리눅스 컨테이너 기술 기반. 
- 경량 이미지로 실행환경을 통째로 백업, 실행 가능 (실무에서 많이 씀) . 시스템 환경설정 + 프로그램을 한번에 배포

cf) Java Virual Machine
- 가상머신과는 다르다! 응용프로그램 레벨 가상화이다.
- Java z컴파일러가 bytecode 생성하고 그 파일을 JVM에서 실행.  해석기
- 각 운영체제를 위한 JVM존재


리눅스 운영체제
= 리눅스 커널(OS) + 시스템 프로그램(쉘) + 응용프로그램

시스템 프로그램
- 시스템관리하는, 연관있는 프로그램. 핵심은 쉘.
- 시스템 콜 사용해서 시스템 제어하는 프로그램.
- 각 프로그래밍 언어는 해당 시스템콜 호출하도록 구현되어있음

안드로이드
-맨 밑에는 리눅스 커널이 있다. -> 쉘도 당연 동작된다. 안드로이드 폰을 따로 연결해서 마치 리눅스 pc처럼 쓰기도 한다.
-안드로이드는 자바쓰니깐 javaVM 필요 (런타임),
-프레임워크(플랫폼)
-안드로이드 = 리눅스 os + 안드로이드 플랫폼(프레임웤)
-안드로이드os라는 말은 틀린말! 안드로이드 플랫폼이라하는게 정확.
-왜 리눅스꺼 쓰나? 운영체제는 만들기도 넘 복잡. 리눅스는 무료.소스도 오픈소스.

사물인터넷 IoT(Internet of Things)
- 각종 사물에 센서와 통신 기능을 내장하여 인터넷에 연결하는 기술
- 라즈베리파이 : 굉장히 작음.
- IoT같은 작은 곳에는 os를 받쳐주는 하드웨어가 부족. 그냥 쓸것만 쓰면됨. 가상메모리 멀티프로세스, 보호모드, 파일시스템 그런거 안필요하니깐 그냥 감지만 해서 application에 보내고 communication만하는 간단한 os사용
